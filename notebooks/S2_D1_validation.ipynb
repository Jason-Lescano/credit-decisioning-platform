{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae691fe",
   "metadata": {},
   "source": [
    "# Sprint2: baseline calibration eval (ECE/Brier + reliability plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfca3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "import numpy as np\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db82610",
   "metadata": {},
   "source": [
    "# 1) Obtención y Calidad de la información"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42ca74",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc85e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"pyproject.toml\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Repo root not found (pyproject.toml missing).\")\n",
    "\n",
    "ROOT = find_repo_root(Path.cwd())\n",
    "DATA_PATH = ROOT / \"data\" / \"processed\" / \"train.parquet\"\n",
    "\n",
    "print(\"Repo root:\", ROOT)\n",
    "print(\"Data path:\", DATA_PATH)\n",
    "print(\"Exists:\", DATA_PATH.exists())\n",
    "\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep issue_d for temporal validation (OOT)\n",
    "if \"issue_d\" not in df.columns:\n",
    "    raise KeyError(\"issue_d not found in raw dataset. Needed for OOT validation.\")\n",
    "\n",
    "df[\"issue_d\"] = pd.to_datetime(df[\"issue_d\"], format=\"%b-%Y\", errors=\"coerce\")\n",
    "\n",
    "parse_rate = df[\"issue_d\"].notna().mean()\n",
    "if parse_rate < 0.95:\n",
    "    raise ValueError(f\"issue_d parse_rate too low: {parse_rate:.3f}. Check raw format.\")\n",
    "\n",
    "# Optional: month bucket for easy plots (YYYY-MM)\n",
    "df[\"issue_month\"] = df[\"issue_d\"].dt.to_period(\"M\").astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde8195f",
   "metadata": {},
   "source": [
    "## Check de variable de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [\n",
    "    c for c in df.columns\n",
    "    if any(k in c.lower() for k in [\"issue\", \"origination\", \"date\", \"time\", \"month\", \"year\"])\n",
    "]\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdedd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa6e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rango de tiempo de la información\n",
    "df[\"issue_d\"] = pd.to_datetime(df[\"issue_d\"], format=\"%b-%Y\", errors=\"coerce\")\n",
    "df[\"issue_d\"].min(), df[\"issue_d\"].max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe738fb",
   "metadata": {},
   "source": [
    "## Observaciones por mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778d0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ensure issue_d is datetime (if it's already datetime64[ns], this is safe) ---\n",
    "df[\"issue_d\"] = pd.to_datetime(df[\"issue_d\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"issue_d\"])\n",
    "\n",
    "# --- Create month key (cohort/vintage) ---\n",
    "df[\"issue_month\"] = df[\"issue_d\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "# --- Cohort size per month ---\n",
    "cohort_size = (\n",
    "    df.groupby(\"issue_month\")\n",
    "      .size()\n",
    "      .rename(\"n_loans\")\n",
    "      .reset_index()\n",
    "      .sort_values(\"issue_month\")\n",
    ")\n",
    "\n",
    "cohort_size.head(), cohort_size.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f26824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cohort size\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(cohort_size[\"issue_month\"], cohort_size[\"n_loans\"])\n",
    "ax.set_title(\"Cohort size by issue month\")\n",
    "ax.set_xlabel(\"Issue month\")\n",
    "ax.set_ylabel(\"Number of loans\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a5ae78",
   "metadata": {},
   "source": [
    "## Default por mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5917755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default rate por mes\n",
    "cohort_default = (\n",
    "    df.groupby(\"issue_month\")[\"target\"]\n",
    "      .agg(default_rate=\"mean\", n_loans=\"size\")\n",
    "      .reset_index()\n",
    "      .sort_values(\"issue_month\")\n",
    ")\n",
    "\n",
    "cohort_default.head(), cohort_default.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bdc6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot default rate\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(cohort_default[\"issue_month\"], cohort_default[\"default_rate\"])\n",
    "ax.set_title(\"Default rate by issue month\")\n",
    "ax.set_xlabel(\"Issue month\")\n",
    "ax.set_ylabel(\"Default rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe46a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(10, 7), sharex=True)\n",
    "\n",
    "# Top: default rate\n",
    "ax1.plot(cohort_default[\"issue_month\"], cohort_default[\"default_rate\"])\n",
    "ax1.set_title(\"Default rate & cohort size by issue month\")\n",
    "ax1.set_ylabel(\"Default rate\")\n",
    "\n",
    "# Bottom: cohort size\n",
    "ax2.plot(cohort_size[\"issue_month\"], cohort_size[\"n_loans\"])\n",
    "ax2.set_xlabel(\"Issue month\")\n",
    "ax2.set_ylabel(\"Number of loans (log)\")\n",
    "\n",
    "# Opcional: ayuda mucho cuando hay meses con tamaños muy distintos\n",
    "ax2.set_yscale(\"log\")  # comenta esta línea si prefieres escala lineal\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7cf51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suavido de la tasa (rolling “pooling” ponderado)\n",
    "tmp = cohort_default.copy()\n",
    "\n",
    "# Rolling 6-month pooled default rate (weighted by n_loans)\n",
    "window = 6\n",
    "tmp[\"default_rate_roll6\"] = (\n",
    "    (tmp[\"default_rate\"] * tmp[\"n_loans\"]).rolling(window, min_periods=3).sum()\n",
    "    / tmp[\"n_loans\"].rolling(window, min_periods=3).sum()\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(tmp[\"issue_month\"], tmp[\"default_rate\"], alpha=0.4)\n",
    "ax.plot(tmp[\"issue_month\"], tmp[\"default_rate_roll6\"])\n",
    "ax.set_title(\"Default rate by issue month (raw vs 6m pooled)\")\n",
    "ax.set_xlabel(\"Issue month\")\n",
    "ax.set_ylabel(\"Default rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae08e7b",
   "metadata": {},
   "source": [
    "# 2) Temporal split (Train / Val / Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed9bef",
   "metadata": {},
   "source": [
    "## Tamaño de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da42c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Monthly cohort table (para elegir cortes por volumen, no por fechas arbitrarias) ---\n",
    "cohort = (\n",
    "    df.groupby(\"issue_month\")[\"target\"]\n",
    "      .agg(n_loans=\"size\", default_rate=\"mean\")\n",
    "      .reset_index()\n",
    "      .sort_values(\"issue_month\")\n",
    ")\n",
    "cohort[\"cum_loans\"] = cohort[\"n_loans\"].cumsum()\n",
    "total = cohort[\"cum_loans\"].iloc[-1]\n",
    "\n",
    "# 70% train, 15% val, 15% test (por cantidad de loans)\n",
    "train_end = cohort.loc[cohort[\"cum_loans\"] <= total * 0.70, \"issue_month\"].max()\n",
    "val_end   = cohort.loc[cohort[\"cum_loans\"] <= total * 0.85, \"issue_month\"].max()\n",
    "\n",
    "print(\"Cutoffs:\")\n",
    "print(\"  train_end =\", train_end)\n",
    "print(\"  val_end   =\", val_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_df = df[df[\"issue_month\"] <= train_end].copy()\n",
    "val_df   = df[(df[\"issue_month\"] > train_end) & (df[\"issue_month\"] <= val_end)].copy()\n",
    "test_df  = df[df[\"issue_month\"] > val_end].copy()\n",
    "\n",
    "def _summ(name, d):\n",
    "    return {\n",
    "        \"split\": name,\n",
    "        \"rows\": len(d),\n",
    "        \"min_month\": d[\"issue_month\"].min(),\n",
    "        \"max_month\": d[\"issue_month\"].max(),\n",
    "        \"default_rate\": float(d[\"target\"].mean()),\n",
    "    }\n",
    "\n",
    "pd.DataFrame([_summ(\"train\", train_df), _summ(\"val\", val_df), _summ(\"test\", test_df)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b0826",
   "metadata": {},
   "source": [
    "# 3) Entrenar y evaluar baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0d8c1",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6138502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Features/target (IMPORTANTE: quitamos issue_d/issue_month) ---\n",
    "DROP_COLS = [\"target\", \"issue_d\", \"issue_month\"]\n",
    "\n",
    "X_train = train_df.drop(columns=DROP_COLS)\n",
    "y_train = train_df[\"target\"].astype(int)\n",
    "\n",
    "X_val = val_df.drop(columns=DROP_COLS)\n",
    "y_val = val_df[\"target\"].astype(int)\n",
    "\n",
    "X_test = test_df.drop(columns=DROP_COLS)\n",
    "y_test = test_df[\"target\"].astype(int)\n",
    "\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# --- Baseline LGBM (simple pero sólido) ---\n",
    "model = lgb.LGBMClassifier(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", model),\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "def eval_split(name, X, y):\n",
    "    p = clf.predict_proba(X)[:, 1]\n",
    "    return {\n",
    "        \"split\": name,\n",
    "        \"auc\": roc_auc_score(y, p),\n",
    "        \"brier\": brier_score_loss(y, p),\n",
    "        \"n\": len(y),\n",
    "        \"default_rate\": float(y.mean()),\n",
    "    }, p\n",
    "\n",
    "rows = []\n",
    "_, p_train = eval_split(\"train\", X_train, y_train)\n",
    "r_val, p_val = eval_split(\"val\", X_val, y_val)\n",
    "r_test, p_test = eval_split(\"test\", X_test, y_test)\n",
    "\n",
    "rows.append(r_val)\n",
    "rows.append(r_test)\n",
    "pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d790dd",
   "metadata": {},
   "source": [
    "## Reliability diagram (calibración) en TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8557374",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_true, prob_pred = calibration_curve(y_test, p_test, n_bins=10, strategy=\"quantile\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.title(\"Calibration (reliability diagram) - TEST (OOT)\")\n",
    "plt.xlabel(\"Mean predicted probability\")\n",
    "plt.ylabel(\"Empirical default rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ec3c7",
   "metadata": {},
   "source": [
    "# 4) Calibración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d5f1e",
   "metadata": {},
   "source": [
    "## Métricas base y función auxiliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9770be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_probs(name, y, p):\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"auc\": roc_auc_score(y, p),\n",
    "        \"brier\": brier_score_loss(y, p),\n",
    "        \"n\": len(y),\n",
    "        \"default_rate\": float(np.mean(y)),\n",
    "        \"avg_pred\": float(np.mean(p)),\n",
    "    }\n",
    "\n",
    "def expected_calibration_error(y, p, n_bins=10):\n",
    "    # ECE estándar: binning uniforme en [0,1]\n",
    "    y = np.asarray(y)\n",
    "    p = np.asarray(p)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    idx = np.digitize(p, bins) - 1\n",
    "    ece = 0.0\n",
    "    for b in range(n_bins):\n",
    "        mask = idx == b\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        acc = y[mask].mean()\n",
    "        conf = p[mask].mean()\n",
    "        w = mask.mean()\n",
    "        ece += w * abs(acc - conf)\n",
    "    return float(ece)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5fd0c",
   "metadata": {},
   "source": [
    "## Baseline probs (sin calibrar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val_base = clf.predict_proba(X_val)[:, 1]\n",
    "p_test_base = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "rows = []\n",
    "rows.append(eval_probs(\"baseline\", y_val, p_val_base) | {\"split\": \"val\", \"ece\": expected_calibration_error(y_val, p_val_base)})\n",
    "rows.append(eval_probs(\"baseline\", y_test, p_test_base) | {\"split\": \"test\", \"ece\": expected_calibration_error(y_test, p_test_base)})\n",
    "\n",
    "pd.DataFrame(rows)[[\"split\",\"model\",\"auc\",\"brier\",\"ece\",\"default_rate\",\"avg_pred\",\"n\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3580fca",
   "metadata": {},
   "source": [
    "## Platt & Isotonic (calibrar en VAL, evaluar en TEST OOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platt scaling (sigmoid)\n",
    "cal_sigmoid = CalibratedClassifierCV(estimator=clf, method=\"sigmoid\", cv=\"prefit\")\n",
    "cal_sigmoid.fit(X_val, y_val)\n",
    "\n",
    "p_val_sig = cal_sigmoid.predict_proba(X_val)[:, 1]\n",
    "p_test_sig = cal_sigmoid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Isotonic\n",
    "cal_iso = CalibratedClassifierCV(estimator=clf, method=\"isotonic\", cv=\"prefit\")\n",
    "cal_iso.fit(X_val, y_val)\n",
    "\n",
    "p_val_iso = cal_iso.predict_proba(X_val)[:, 1]\n",
    "p_test_iso = cal_iso.predict_proba(X_test)[:, 1]\n",
    "\n",
    "rows = []\n",
    "for split_name, y, p_base, p_sig, p_iso in [\n",
    "    (\"val\", y_val, p_val_base, p_val_sig, p_val_iso),\n",
    "    (\"test\", y_test, p_test_base, p_test_sig, p_test_iso),\n",
    "]:\n",
    "    rows.append(eval_probs(\"baseline\", y, p_base) | {\"split\": split_name, \"ece\": expected_calibration_error(y, p_base)})\n",
    "    rows.append(eval_probs(\"platt_sigmoid\", y, p_sig) | {\"split\": split_name, \"ece\": expected_calibration_error(y, p_sig)})\n",
    "    rows.append(eval_probs(\"isotonic\", y, p_iso) | {\"split\": split_name, \"ece\": expected_calibration_error(y, p_iso)})\n",
    "\n",
    "df_metrics = pd.DataFrame(rows)[[\"split\",\"model\",\"auc\",\"brier\",\"ece\",\"default_rate\",\"avg_pred\",\"n\"]]\n",
    "df_metrics.sort_values([\"split\",\"model\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11ec89",
   "metadata": {},
   "source": [
    "## Curvas de calibración (antes vs después) en TEST OOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECE\n",
    "def expected_calibration_error(y_true, p_pred, n_bins=10, strategy=\"quantile\"):\n",
    "    \"\"\"\n",
    "    ECE (Expected Calibration Error):\n",
    "    sum_b (|b|/N) * |acc(b) - conf(b)|\n",
    "    where acc(b)=mean(y), conf(b)=mean(p) within bin.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    p_pred = np.asarray(p_pred)\n",
    "\n",
    "    df_ = pd.DataFrame({\"y\": y_true, \"p\": p_pred})\n",
    "\n",
    "    if strategy == \"quantile\":\n",
    "        # bins con igual cantidad de observaciones (maneja duplicados con drop)\n",
    "        df_[\"bin\"] = pd.qcut(df_[\"p\"], q=n_bins, duplicates=\"drop\")\n",
    "    elif strategy == \"uniform\":\n",
    "        df_[\"bin\"] = pd.cut(df_[\"p\"], bins=n_bins, include_lowest=True)\n",
    "    else:\n",
    "        raise ValueError(\"strategy must be 'quantile' or 'uniform'\")\n",
    "\n",
    "    g = df_.groupby(\"bin\", observed=False)\n",
    "    bin_stats = g.agg(\n",
    "        n=(\"p\", \"size\"),\n",
    "        conf=(\"p\", \"mean\"),\n",
    "        acc=(\"y\", \"mean\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    bin_stats[\"w\"] = bin_stats[\"n\"] / len(df_)\n",
    "    bin_stats[\"abs_gap\"] = (bin_stats[\"acc\"] - bin_stats[\"conf\"]).abs()\n",
    "\n",
    "    ece = float((bin_stats[\"w\"] * bin_stats[\"abs_gap\"]).sum())\n",
    "    return ece, bin_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8378b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de la ECE\n",
    "ece_base, bins_base = expected_calibration_error(y_test, p_test_base, n_bins=10, strategy=\"quantile\")\n",
    "ece_platt, bins_platt = expected_calibration_error(y_test, p_test_sig, n_bins=10, strategy=\"quantile\")\n",
    "ece_iso, bins_iso = expected_calibration_error(y_test, p_test_iso, n_bins=10, strategy=\"quantile\")\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"model\": [\"baseline\", \"platt_sigmoid\", \"isotonic\"],\n",
    "    \"ece_quantile_10bins\": [ece_base, ece_platt, ece_iso],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df101bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_calibration(y, probs_dict, title, n_bins=10):\n",
    "    plt.figure()\n",
    "    plt.plot([0, 1], [0, 1], \"--\")  # perfect calibration line\n",
    "\n",
    "    for name, p in probs_dict.items():\n",
    "        frac_pos, mean_pred = calibration_curve(y, p, n_bins=n_bins, strategy=\"quantile\")\n",
    "        plt.plot(mean_pred, frac_pos, marker=\"o\", label=name)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.ylabel(\"Empirical default rate\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_calibration(\n",
    "    y_test,\n",
    "    {\n",
    "        \"baseline\": p_test_base,\n",
    "        \"platt_sigmoid\": p_test_sig,\n",
    "        \"isotonic\": p_test_iso,\n",
    "    },\n",
    "    title=\"Calibration (reliability diagram) - TEST (OOT)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a0cce4",
   "metadata": {},
   "source": [
    "# 5) Cierre del baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44904787",
   "metadata": {},
   "source": [
    "## Guardar métricas baseline y calibrados en un artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc5a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Path(\"artifacts/reports\")\n",
    "out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# df_metrics ya lo tienes (split/model/auc/brier/ece/...)\n",
    "df_metrics.to_csv(out / \"s2_baseline_calibration_metrics.csv\", index=False)\n",
    "\n",
    "with open(out / \"s2_baseline_notes.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"note\": \"Baseline + calibration on VAL; evaluated on TEST (OOT).\",\n",
    "        \"calibration_bins\": 10,\n",
    "        \"calibration_strategy\": \"quantile\",\n",
    "        \"models\": [\"baseline\", \"platt_sigmoid\", \"isotonic\"],\n",
    "    }, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb161ef",
   "metadata": {},
   "source": [
    "## Anity check de “avg_pred vs default_rate” (por split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd53beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.assign(gap=lambda d: d[\"avg_pred\"] - d[\"default_rate\"])[\n",
    "    [\"split\",\"model\",\"default_rate\",\"avg_pred\",\"gap\",\"auc\",\"brier\",\"ece\",\"n\"]\n",
    "].sort_values([\"split\",\"model\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
